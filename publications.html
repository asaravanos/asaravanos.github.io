<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <title>Publications - Augustinos Saravanos</title>
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet">
  <link rel="stylesheet" href="/assets/css/main.css">
</head>
<body class="fixed-top-nav">
<nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
  <div class="container">
    <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse"
            data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false"
            aria-label="Toggle navigation"><span class="sr-only">Toggle navigation</span> <span
            class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span
            class="icon-bar bottom-bar"></span></button>
    <div class="collapse navbar-collapse text-right" id="navbarNav">
      <ul class="navbar-nav ml-auto flex-nowrap">
        <li class="nav-item"><a class="nav-link" href="index.html">about</a>
        </li>
        <li class="nav-item"><a class="nav-link"
                                href="https://asaravanos.github.io/assets/pdf/Saravanos_CV.pdf"
                                target="\_blank">CV</a></li>
        <li  class="nav-item active" >
          <a class="nav-link" href="publications.html" target="_self">Publications<span class="sr-only">(current)</span></a>
        </li>
        <li class="toggle-container">
          <button id="light-toggle" title="Change theme"><i class="fas fa-moon"></i> <i
                  class="fas fa-sun"></i></button>
        </li>
      </ul>
    </div>
  </div>
</nav>

  <div class="container mt-5">
    <h1 class="text-center">Publications</h1>
    <div class="publications" style="max-width: 120vw; width: 100%;">
      <p class="intro-text">
        * Equal contribution. See <a href="https://scholar.google.com/citations?user=6XP9s1MAAAAJ&hl=en">Google
        Scholar</a> for most up-to-date list.
      </p>

  <h4 class="text-left">Preprints / Under Review</h4>
      <ol class="bibliography">
        <li>
          <div>
            <div class="col-sm-2 abbr"><abbr class="badge" style="width: 120px;"> Preprint </abbr></div>
            <div id="abdul2024robust">
              <div class="title"><font size="+1">Scaling Robust Optimization for Multi-Agent Robotic
                Systems: A Distributed Perspective</font></div>
              <div class="author"> A.T. Abdul*, <strong><font color="#A0522D">A.D. Saravanos*</font></strong> and E.A. Theodorou
              </div>
              <div class="periodical"><font color="#3B57D3">Preprint (Under review)</font>, 2024.
              </div>
              <div class="links"><a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
                <a href="https://arxiv.org/abs/2402.16227" class="btn btn-sm z-depth-0"
                   role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a
                        href="https://www.youtube.com/watch?v=yrDwPwg4WFI"
                        class="btn btn-sm z-depth-0" role="button" target="_blank"
                        rel="noopener noreferrer">Video</a></div>
              <div class="abstract hidden"><p>This paper presents a novel distributed robust
                optimization scheme for steering distributions of multi-agent systems under
                stochastic and deterministic uncertainty. Robust optimization is a subfield of
                optimization which aims in discovering an optimal solution that remains robustly
                feasible for all possible realizations of the problem parameters within a given
                uncertainty set. Such approaches would naturally constitute an ideal candidate for
                multi-robot control, where in addition to stochastic noise, there might be exogenous
                deterministic disturbances. Nevertheless, as these methods are usually associated
                with significantly high computational demands, their application to multi-agent
                robotics has remained limited. The scope of this work is to propose a scalable
                robust optimization framework that effectively addresses both types of
                uncertainties, while retaining computational efficiency and scalability. In this
                direction, we provide tractable approximations for robust constraints that relevant
                in multi-robot settings. Subsequently, we demonstrate how computations can be
                distributed through an Alternating Direction Method of Multipliers (ADMM) approach
                towards achieving scalability and communication efficiency. Simulation results
                highlight the performance of the proposed algorithm in effectively handling both
                stochastic and deterministic uncertainty in multi-robot systems. The scalability of
                the method is also emphasized by showcasing tasks with up to 100 agents. The results
                of this work indicate the promise of blending robust optimization, distribution
                steering and distributed optimization towards achieving scalable, safe and robust
                multi-robot control.</p></div>
            </div>
          </div>
        </li>

        <li>
          <div>
            <div class="col-sm-2 abbr"><abbr class="badge" style="width: 120px;"> Preprint </abbr></div>
            <div id="abdul2024robust">
              <div class="title"><font size="+1">Second-Order Constrained Dynamic Optimization</font></div>
              <div class="author"> Y. Aoyama, O. So, <strong><font color="#A0522D">A.D. Saravanos</font></strong> and E.A. Theodorou
              </div>
              <div class="periodical"><font color="#3B57D3">Preprint (Under review)</font>, 2024.
              </div>
              <div class="links"><a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
                <a href="https://arxiv.org/abs/2409.11649" class="btn btn-sm z-depth-0"
                   role="button" target="_blank" rel="noopener noreferrer">PDF</a></div>
              <div class="abstract hidden"><p>This paper presents a novel distributed robust
                optimization scheme for steering distributions of multi-agent systems under
                stochastic and deterministic uncertainty. Robust optimization is a subfield of
                optimization which aims in discovering an optimal solution that remains robustly
                feasible for all possible realizations of the problem parameters within a given
                uncertainty set. Such approaches would naturally constitute an ideal candidate for
                multi-robot control, where in addition to stochastic noise, there might be exogenous
                deterministic disturbances. Nevertheless, as these methods are usually associated
                with significantly high computational demands, their application to multi-agent
                robotics has remained limited. The scope of this work is to propose a scalable
                robust optimization framework that effectively addresses both types of
                uncertainties, while retaining computational efficiency and scalability. In this
                direction, we provide tractable approximations for robust constraints that relevant
                in multi-robot settings. Subsequently, we demonstrate how computations can be
                distributed through an Alternating Direction Method of Multipliers (ADMM) approach
                towards achieving scalability and communication efficiency. Simulation results
                highlight the performance of the proposed algorithm in effectively handling both
                stochastic and deterministic uncertainty in multi-robot systems. The scalability of
                the method is also emphasized by showcasing tasks with up to 100 agents. The results
                of this work indicate the promise of blending robust optimization, distribution
                steering and distributed optimization towards achieving scalable, safe and robust
                multi-robot control.</p></div>
            </div>
          </div>
        </li>

        <li>
          <div>
            <div class="col-sm-2 abbr"><abbr class="badge" style="width: 120px;"> Preprint </abbr></div>
            <div id="abdul2024robust">
              <div class="title"><font size="+1">Operator Splitting Covariance Steering for Safe Stochastic Nonlinear Control</font></div>
              <div class="author"> A. Ratheesh, V. Pacelli, <strong><font color="#A0522D">A.D. Saravanos</font></strong> and E.A. Theodorou
              </div>
              <div class="periodical"><font color="#3B57D3">Preprint (Under review)</font>, 2024.
              </div>
              <div class="links"><a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
                <a href="https://arxiv.org/abs/2411.11211" class="btn btn-sm z-depth-0"
                   role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a
                        href="https://www.youtube.com/watch?v=UCyYcDITO2Q"
                        class="btn btn-sm z-depth-0" role="button" target="_blank"
                        rel="noopener noreferrer">Video</a></div>
              <div class="abstract hidden"><p>This paper presents a novel distributed robust
                optimization scheme for steering distributions of multi-agent systems under
                stochastic and deterministic uncertainty. Robust optimization is a subfield of
                optimization which aims in discovering an optimal solution that remains robustly
                feasible for all possible realizations of the problem parameters within a given
                uncertainty set. Such approaches would naturally constitute an ideal candidate for
                multi-robot control, where in addition to stochastic noise, there might be exogenous
                deterministic disturbances. Nevertheless, as these methods are usually associated
                with significantly high computational demands, their application to multi-agent
                robotics has remained limited. The scope of this work is to propose a scalable
                robust optimization framework that effectively addresses both types of
                uncertainties, while retaining computational efficiency and scalability. In this
                direction, we provide tractable approximations for robust constraints that relevant
                in multi-robot settings. Subsequently, we demonstrate how computations can be
                distributed through an Alternating Direction Method of Multipliers (ADMM) approach
                towards achieving scalability and communication efficiency. Simulation results
                highlight the performance of the proposed algorithm in effectively handling both
                stochastic and deterministic uncertainty in multi-robot systems. The scalability of
                the method is also emphasized by showcasing tasks with up to 100 agents. The results
                of this work indicate the promise of blending robust optimization, distribution
                steering and distributed optimization towards achieving scalable, safe and robust
                multi-robot control.</p></div>
            </div>
          </div>
        </li>
      </ol>

  <h4 class="text-left">Journal Papers</h4>
      <ol class="bibliography">
        <li>
          <div>
            <div class="col-sm-2 abbr"><abbr class="badge" style="width: 120px;">IEEE Transactions <br>
              on Robotics </abbr></div>
            <div id="saravanos2022distributed_ddp">
              <div class="title"><font size="+1">Distributed Differential Dynamic Programming
                Architectures for Large-Scale Multi-Agent Control</font></div>
              <div class="author"><strong><font color="#A0522D">A.D. Saravanos</font></strong>, Y.
                Aoyama, H. Zhu, and E. A. Theodorou
              </div>
              <div class="periodical"><strong><font color="#3B57D3">IEEE Transactions on Robotics</font>
              </strong>, 2023.</div>
              <div class="links"><a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
                <a href="https://arxiv.org/abs/2207.13255" class="btn btn-sm z-depth-0"
                   role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a
                        href="https://www.youtube.com/watch?v=tluvENcWldw"
                        class="btn btn-sm z-depth-0" role="button" target="_blank"
                        rel="noopener noreferrer">Video</a></div>
              <div class="abstract hidden"><p>In this paper, we propose two novel decentralized
                optimization frameworks for multi-agent nonlinear optimal control problems in
                robotics. The aim of this work is to suggest architectures that inherit the
                computational efficiency and scalability of Differential Dynamic Programming (DDP)
                and the distributed nature of the Alternating Direction Method of Multipliers
                (ADMM). In this direction, two frameworks are introduced. The first one called
                Nested Distributed DDP (ND-DDP), is a three-level architecture which employs ADMM
                for enforcing a consensus between all agents, an Augmented Lagrangian layer for
                satisfying local constraints and DDP as each agent’s optimizer. In the second
                approach, both consensus and local constraints are handled with ADMM, yielding a
                two-level architecture called Merged Distributed DDP (MD-DDP), which further reduces
                computational complexity. Both frameworks are fully decentralized since all
                computations are parallelizable among the agents and only local communication is
                necessary. Simulation results that scale up to thousands of vehicles and hundreds of
                drones verify the effectiveness of the methods. Superior scalability to large-scale
                systems against centralized DDP and centralized/decentralized sequential quadratic
                programming is also illustrated. Finally, hardware experiments on a multi-robot
                platform demonstrate the applicability of the proposed algorithms, while
                highlighting the importance of optimizing for feedback policies to increase
                robustness against uncertainty. <a href="https://youtu.be/tluvENcWldw"
                                                   target="_blank" rel="noopener noreferrer">A video
                  with all results is available here</a>.</p></div>
            </div>
          </div>
        </li>
      </ol>

  <h4 class="text-left">Conference Papers</h4>
      
      <ol class="bibliography">
        <li>
          <div>
            <div class="col-sm-2 abbr"><abbr class="badge" style="width: 120px;"> Preprint </abbr></div>
            <div id="saravanos2024deep">
              <div class="title"><font size="+1">Deep Distributed Optimization for Large-Scale
                Quadratic Programming</font></div>
              <div class="author"><strong><font color="#A0522D">A.D. Saravanos</font></strong>, H.
                Kuperman, A. Oshin, A.T. Abdul, V. Pacelli and E.A. Theodorou
              </div>
              <div class="periodical"><strong><font color="#3B57D3">International Conference on Learning Representations (ICLR)</font></strong>, 2025.
              </div>
              <div class="links"><a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
                <a href="https://arxiv.org/abs/2412.12156" class="btn btn-sm z-depth-0"
                   role="button" target="_blank" rel="noopener noreferrer">PDF</a></div>
              <div class="abstract hidden"><p>Quadratic programming (QP) forms a crucial foundation in
                optimization, encompassing a broad spectrum of domains and serving as the basis for
                more advanced algorithms. Consequently, as the scale and complexity of modern
                applications continue to grow, the development of efficient and reliable QP
                algorithms is becoming increasingly vital. In this context, this paper introduces a
                novel deep learning-aided distributed optimization architecture designed for
                tackling large-scale QP problems. First, we combine the state-of-the-art Operator
                Splitting QP (OSQP) method with a consensus approach to derive DistributedQP, a new
                method tailored for network-structured problems, with convergence guarantees to
                optimality. Subsequently, we unfold this optimizer into a deep learning framework,
                leading to DeepDistributedQP, which leverages learned policies to accelerate
                reaching to desired accuracy within a restricted amount of iterations. Our approach
                is also theoretically grounded through Probably Approximately Correct (PAC)-Bayes
                theory, providing generalization bounds on the expected optimality gap for unseen
                problems. The proposed framework, as well as its centralized version DeepQP,
                significantly outperform their standard optimization counterparts on a variety of
                tasks such as randomly generated problems, optimal control, linear regression,
                transportation networks and others. Notably, DeepDistributedQP demonstrates strong
                generalization by training on small problems and scaling to solve much larger ones
                (up to 50K variables and 150K constraints) using the same policy. Moreover, it
                achieves orders-of-magnitude improvements in wall-clock time compared to OSQP. The
                certifiable performance guarantees of our approach are also demonstrated, ensuring
                higher-quality solutions over traditional optimizers.</p></div>
            </div>
          </div>
        </li>

        <li>
          <div>
            <div class="col-sm-2 abbr"><abbr class="badge" style="width: 120px;"> Preprint </abbr></div>
            <div id="abdul2025scalable">
              <div class="title"><font size="+1">Scalable Robust Optimization for Safe Multi-Agent Control Under Deterministic Uncertainty</font></div>
              <div class="author"> A.T. Abdul*, <strong><font color="#A0522D">A.D. Saravanos</font></strong> and E.A. Theodorou
              </div>
              <div class="periodical"><strong><font color="#3B57D3">American Control Conference (ACC)</font></strong>, 2025.
              </div>
              <div class="links"><a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
                <a href="https://arxiv.org/abs/2412.12156" class="btn btn-sm z-depth-0"
                   role="button" target="_blank" rel="noopener noreferrer">PDF</a></div>
              <div class="abstract hidden"><p>This paper introduces a novel framework for addressing multi-agent trajectory optimization under unknown deterministic uncertainty. 
                Many systems are affected by deterministic disturbances, such as environmental effects, system degradation, etc., which cannot be accurately modeled using stochastic 
                signals. Therefore, it is crucial to develop trajectory optimization frameworks that ensure safety despite these disturbances. To this end, we focus on solving a 
                multi-agent trajectory optimization problem involving \textit{robust} constraints, such as collision avoidance, that must be satisfied for all possible realizations 
                of uncertainty lying in an ellipsoidal set. Conventional robust optimization techniques that are used to address such problems are computationally expensive and struggle 
                when dealing with numerous constraints. To overcome this, we propose tighter approximations of robust constraints that significantly reduce computational complexity 
                without compromising safety. Furthermore, leveraging these constraint approximations, we introduce a distributed robust optimization framework for decentralized multi-agent 
                robust trajectory optimization based on Alternating Direction Method of Multipliers (ADMM). This framework allows agents to optimize their trajectories without sharing 
                control parameters or system information (e.g., dynamics), thereby preserving data security. The effectiveness of the proposed robust constraint approximations and the 
                scalability of the proposed distributed framework are demonstrated through simulation data.</p></div>
            </div>
          </div>
        </li>

        <li>
          <div>
            <div class="col-sm-2 abbr"><abbr class="badge" style="width: 120px;">IROS 2024 </abbr></div>
            <div id="saravanos2022distributed_mpcs">
              <div class="title"><font size="+1">Distributed Model Predictive Covariance
                Steering</font></div>
              <div class="author"><strong><font color="#A0522D">A.D. Saravanos</font></strong>, I.M.
                Balci, E. Bakolas, and E.A. Theodorou
              </div>
              <div class="periodical"><strong><font color="#3B57D3">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</font></strong>, 2024
              </div>
              <div class="links"><a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
                <a href="https://arxiv.org/abs/2212.00398" class="btn btn-sm z-depth-0"
                   role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a
                        href="https://www.youtube.com/watch?v=Hks-0BRozxA"
                        class="btn btn-sm z-depth-0" role="button" target="_blank"
                        rel="noopener noreferrer">Video</a></div>
              <div class="abstract hidden"><p>This paper proposes Distributed Model Predictive
                Covariance Steering (DMPCS), a novel method for safe multi-robot control under
                uncertainty. The scope of our approach is to blend covariance steering theory,
                distributed optimization and model predictive control (MPC) into a single
                methodology that is safe, scalable and decentralized. Initially, we pose a problem
                formulation that uses the Wasserstein distance to steer the state distributions of a
                multi-robot team to desired targets, and probabilistic constraints to ensure safety.
                We then transform this problem into a finite-dimensional optimization one by
                utilizing a disturbance feedback policy parametrization for covariance steering and
                a tractable approximation of the safety constraints. To solve the latter problem, we
                derive a decentralized consensus-based algorithm using the Alternating Direction
                Method of Multipliers (ADMM). This method is then extended to a receding horizon
                form, which yields the proposed DMPCS algorithm. Simulation experiments on
                large-scale problems with up to hundreds of robots successfully demonstrate the
                effectiveness and scalability of DMPCS. Its superior capability in achieving safety
                is also highlighted through a comparison against a standard stochastic MPC
                approach.</p></div>
            </div>
          </div>
        </li>

        <li>
          <div>
            <div class="col-sm-2 abbr"><abbr class="badge" style="width: 120px;">IROS 2024 </abbr></div>
            <div id="saravanos2022distributed_mpcs">
              <div class="title"><font size="+1">A Robust Differential Neural ODE Optimizer</font></div>
              <div class="author">P. Theodoropoulos, G.H. Liu, T. Chen, <strong><font color="#A0522D">A.D. Saravanos</font></strong> and E.A. Theodorou
              </div>
              <div class="periodical"><strong><font color="#3B57D3">International Conference on Learning Representations (ICLR)</font></strong>, 2024
              </div>
              <div class="links"><a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
                <a href="https://openreview.net/forum?id=zbOSJ3CATY" class="btn btn-sm z-depth-0"
                   role="button" target="_blank" rel="noopener noreferrer">PDF</a> </div>
              <div class="abstract hidden"><p>Neural networks and neural ODEs tend to be vulnerable to adversarial attacks, rendering robust optimizers 
                critical to curb the success of such attacks. In this regard, the key insight of this work is to interpret Neural ODE optimization as a 
                min-max optimal control problem. More particularly, we present Game Theoretic Second-Order Neural Optimizer (GTSONO), a robust game theoretic 
                optimizer based on the principles of min-max Differential Dynamic Programming. The proposed method exhibits significant computational 
                benefits due to efficient matrix decompositions and provides convergence guarantees to local saddle points. Empirically, the robustness of 
                the proposed optimizer is demonstrated through greater robust accuracy compared to benchmark optimizers when trained on clean images. 
                Additionally, its ability to provide a performance increase when adapted to an already existing adversarial defense technique is also illustrated. 
                Finally, the superiority of the proposed update law over its gradient based counterpart highlights the potential benefits of incorporating robust 
                optimal control paradigms into adversarial training methods.</p></div>
            </div>
          </div>
        </li>

        <li>
          <div>
            <div class="col-sm-2 abbr"><abbr class="badge" style="width: 120px;">RSS 2023 </abbr></div>
            <div id="saravanos2023hierarchical">
              <div class="title"><font size="+1">Distributed Hierarchical Distribution Control for
                Very-Large-Scale Clustered Multi-Agent Systems</font></div>
              <div class="author"><strong><font color="#A0522D">A.D. Saravanos</font></strong>, Y.
                Li and E.A. Theodorou
              </div>
              <div class="periodical"><strong><font color="#3B57D3">Robotics: Science and Systems</font></strong>, 2023.</div>
              <div class="links"><a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
                <a href="http://www.roboticsproceedings.org/rss18/p055.html"
                   class="btn btn-sm z-depth-0" role="button" target="_blank"
                   rel="noopener noreferrer">PDF</a> <a
                        href="https://www.youtube.com/watch?v=0QPyR4bD2q0"
                        class="btn btn-sm z-depth-0" role="button" target="_blank"
                        rel="noopener noreferrer">Video</a></div>
              <div class="abstract hidden"><p>As the scale and complexity of multi-agent robotic
                systems are subject to a continuous increase, this paper considers a class of
                systems labeled as Very-Large-Scale Multi-Agent Systems (VLMAS) with dimensionality
                that can scale up to the order of millions of agents. In particular, we consider the
                problem of steering the state distributions of all agents of a VLMAS to prescribed
                target distributions while satisfying probabilistic safety guarantees. Based on the
                key assumption that such systems often admit a multi-level hierarchical clustered
                structure - where the agents are organized into cliques of different levels - we
                associate the control of such cliques with the control of distributions, and
                introduce the Distributed Hierarchical Distribution Control (DHDC) framework. The
                proposed approach consists of two sub-frameworks. The first one, Distributed
                Hierarchical Distribution Estimation (DHDE), is a bottom-up hierarchical
                decentralized algorithm which links the initial and target configurations of the
                cliques of all levels with suitable Gaussian distributions. The second part,
                Distributed Hierarchical Distribution Steering (DHDS), is a top-down hierarchical
                distributed method that steers the distributions of all cliques and agents from the
                initial to the targets ones assigned by DHDE. Simulation results that scale up to
                two million agents demonstrate the effectiveness and scalability of the proposed
                framework. The increased computational efficiency and safety performance of DHDC
                against related methods is also illustrated. The results of this work indicate the
                importance of hierarchical distribution control approaches towards achieving safe
                and scalable solutions for the control of VLMAS.</p></div>
            </div>
          </div>
        </li>


        <li>
          <div>
            <div class="col-sm-2 abbr"><abbr class="badge" style="width: 120px;">IROS 2024 </abbr></div>
            <div id="saravanos2022distributed_mpcs">
              <div class="title"><font size="+1">Distributed Model Predictive Covariance
                Steering</font></div>
              <div class="author">J.E. Kuperman, H. Almubarak, <strong><font color="#A0522D">A.D. Saravanos</font></strong> and E.A. Theodorou
              </div>
              <div class="periodical"><strong><font color="#3B57D3">International Conference on Advanced Robotics (ICAR)</font></strong>, 2023.
              </div>
              <div class="links"><a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
                <a href="https://ieeexplore.ieee.org/abstract/document/10406518" class="btn btn-sm z-depth-0"
                   role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a
                        href="https://www.youtube.com/watch?v=9ZRBHZfjKPY"
                        class="btn btn-sm z-depth-0" role="button" target="_blank"
                        rel="noopener noreferrer">Video</a></div>
              <div class="abstract hidden"><p>This paper proposes Distributed Model Predictive
                Covariance Steering (DMPCS), a novel method for safe multi-robot control under
                uncertainty. The scope of our approach is to blend covariance steering theory,
                distributed optimization and model predictive control (MPC) into a single
                methodology that is safe, scalable and decentralized. Initially, we pose a problem
                formulation that uses the Wasserstein distance to steer the state distributions of a
                multi-robot team to desired targets, and probabilistic constraints to ensure safety.
                We then transform this problem into a finite-dimensional optimization one by
                utilizing a disturbance feedback policy parametrization for covariance steering and
                a tractable approximation of the safety constraints. To solve the latter problem, we
                derive a decentralized consensus-based algorithm using the Alternating Direction
                Method of Multipliers (ADMM). This method is then extended to a receding horizon
                form, which yields the proposed DMPCS algorithm. Simulation experiments on
                large-scale problems with up to hundreds of robots successfully demonstrate the
                effectiveness and scalability of DMPCS. Its superior capability in achieving safety
                is also highlighted through a comparison against a standard stochastic MPC
                approach.</p></div>
            </div>
          </div>
        </li>

        <li>
          <div>
            <div class="col-sm-2 abbr"><abbr class="badge" style="width: 120px;">RSS 2022 </abbr></div>
            <div id="pereira2022decentralized">
              <div class="title"><font size="+1">Decentralized Safe Multi-agent Stochastic Optimal
                Control using Deep FBSDEs and ADMM</font></div>
              <div class="author"> M.A. Pereira*, <strong><font color="#A0522D">A.D. Saravanos*</font></strong>, O. So, and E.A. Theodorou
              </div>
              <div class="periodical"><strong><font color="#3B57D3">Robotics: Science and Systems </font>
              </strong>, 2022.</div>
              <div class="links"><a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
                <a href="http://www.roboticsproceedings.org/rss18/p055.html"
                   class="btn btn-sm z-depth-0" role="button" target="_blank"
                   rel="noopener noreferrer">PDF</a> <a
                        href="https://www.youtube.com/watch?v=qjPLUlaxJos"
                        class="btn btn-sm z-depth-0" role="button" target="_blank"
                        rel="noopener noreferrer">Video</a></div>
              <div class="abstract hidden"><p>In this work, we propose a novel safe and scalable
                decentralized solution for multi-agent control in the presence of stochastic. Safety
                is mathematically encoded using stochastic control barrier functions and safe
                controls are computed by solving quadratic programs. Decentralization is achieved by
                augmenting to each agent’s optimization variables, copy variables, for its
                neighboring agents. This allows us to decouple the centralized multi-agent
                optimization problem. However, to ensure safety, neighboring agents must agree on
                what is safe for both of us and this creates a need for consensus. To enable safe
                consensus solutions, we incorporate an ADMM-based approach. Specifically, we propose
                a Merged CADMM-OSQP implicit neural network layer, that solves a mini-batch of both,
                local quadratic programs as well as the overall consensus problem, as a single
                optimization problem. This layer is embedded within a Deep FBSDEs network
                architecture at every time step, to facilitate end-to-end differentiable, safe and
                decentralized stochastic optimal control. The efficacy of the proposed approach is
                demonstrated on several challenging multi-robot tasks in simulation. By imposing
                requirements on safety specified by collision avoidance constraints, the safe
                operation of all agents is ensured during the entire training process. We also
                demonstrate superior scalability in terms of computational and memory savings as
                compared to a centralized approach.</p></div>
            </div>
          </div>
        </li>

        <li>
          <div>
            <div class="col-sm-2 abbr"><abbr class="badge" style="width: 120px;">CDC 2021 </abbr></div>
            <div id="aoyama2021receding">
              <div class="title"><font size="+1">Receding Horizon Differential Dynamic Programming Under Parametric Uncertainty</font></div>
              <div class="author"> Y. Aoyama, <strong><font color="#A0522D">A.D. Saravanos</font></strong>, and E.A. Theodorou
              </div>
              <div class="periodical"><strong><font color="#3B57D3">IEEE Conference on Decision and Control (CDC)</font>
              </strong> , 2021.</div>
              <div class="links"><a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
                <a href="https://ieeexplore.ieee.org/abstract/document/9683370"
                   class="btn btn-sm z-depth-0" role="button" target="_blank"
                   rel="noopener noreferrer">PDF</a> </div>
              <div class="abstract hidden"><p>Generalized Polynomial Chaos (gPC) theory has been widely used for representing parametric uncertainty in a system, 
                thanks to its ability to propagate uncertainty evolution. In an optimal control context, gPC can be combined with several optimization techniques 
                to achieve a control policy that handles effectively this type of uncertainty. Such a suitable method is Differential Dynamic Programming (DDP), 
                leading to an algorithm that inherits the scalability to high-dimensional systems and fast convergence nature of the latter. In this paper, we expand 
                this combination aiming to acquire probabilistic guarantees on the satisfaction of nonlinear constraints. In particular, we exploit the ability of 
                gPC to express higher order moments of the uncertainty distribution - without any Gaussianity assumption - and we incorporate chance constraints that 
                lead to expressions involving the state covariance. Furthermore, we demonstrate that by implementing our algorithm in a receding horizon fashion, 
                we are able to compute control policies that effectively reduce the accumulation of uncertainty on the trajectory. The applicability of our method is 
                verified through simulation results on a differential wheeled robot and a quadrotor that perform obstacle avoidance tasks.</p></div>
            </div>
          </div>
        </li>
        
        <li>
          <div>
            <div class="col-sm-2 abbr"><abbr class="badge" style="width: 120px;">RSS 2021 </abbr></div>
            <div id="saravanos2021distributed">
              <div class="title"><font size="+1">Distributed Covariance Steering with Consensus ADMM
                for Stochastic Multi-Agent Systems</font></div>
              <div class="author"><strong><font color="#A0522D">A.D. Saravanos</font></strong>, A.
                Tsolovikos, E. Bakolas, and E.A. Theodorou
              </div>
              <div class="periodical"><strong><font color="#3B57D3">Robotics: Science and Systems</font>
              </strong> , 2021.</div>
              <div class="links"><a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
                <a href="http://www.roboticsproceedings.org/rss17/p075.html"
                   class="btn btn-sm z-depth-0" role="button" target="_blank"
                   rel="noopener noreferrer">PDF</a> <a
                        href="https://www.youtube.com/watch?v=RiLQ2P1WXHQ"
                        class="btn btn-sm z-depth-0" role="button" target="_blank"
                        rel="noopener noreferrer">Video</a></div>
              <div class="abstract hidden"><p>In this paper, we address the problem of steering a team
                of agents under stochastic linear dynamics to prescribed final state means and
                covariances. The agents operate in a common environment where inter-agent
                constraints may also be present. In order for our method to be scalable to
                large-scale systems and computationally efficient, we approach the problem in a
                distributed control framework using the Alternating Direction Method of Multipliers
                (ADMM). Each agent solves its own covariance steering problem in parallel, while
                additional copy variables for its closest neighbors are introduced to ensure that
                the inter-agent constraints will be satisfied. The inclusion of these additional
                variables creates a requirement for consensus between original and copy variables
                that involve the same agent. For this reason, we employ a variation of ADMM for
                consensus optimization. Simulation results on multi-vehicle systems under
                uncertainty with collision avoidance constraints illustrate the effectiveness of our
                algorithm. The substantially improved scalability of our distributed approach with
                respect to the number of agents is also demonstrated, in comparison with an
                equivalent centralized scheme.</p></div>
            </div>
          </div>
        </li>

      </ol>
    </div>
  </div>
  <footer>
    <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js"
        integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js"
        integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js"
        integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js"
        integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
    <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js"
        integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
    <script defer src="/assets/js/zoom.js"></script>
    <script defer src="/assets/js/common.js"></script>
  </footer>
</body>
</html>
