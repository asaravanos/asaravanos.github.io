---
---

@article{wang2022sampling_based,
    abbr={JMLR <br> (In Submission)},
    title={Sampling-Based Optimization for Multi-Agent Model Predictive Control},
    author={Wang, Ziyi and Saravanos, Augustinos D. and Almubarak, Hassan and So, Oswin AND Theodorou, Evangelos},
    journal={Journal of Machine Learning Research (In Submission)},
    year={2022},
    selected={true},
    abstract = {We systematically review the Variational Optimization, Variational Inference and Stochastic Search
    perspectives on sampling-based dynamic optimization and discuss their connections to state-of-the-art optimizers
    and Stochastic Optimal Control (SOC) theory. A general convergence and sample complexity analysis on the three
    perspectives is provided through the unifying Stochastic Search perspective. We then extend these frameworks to their
    distributed versions for multi-agent control by combining them with consensus Alternating Direction Method of Multipliers
    (ADMM) to decouple the full problem into local neighborhood-level ones that can be solved in parallel. Model Predictive
    Control (MPC) algorithms are then developed based on these frameworks, leading to fully decentralized sampling-based
    dynamic optimizers. The capabilities of the proposed algorithms framework are demonstrated on multiple complex multi-agent
    tasks for vehicle and quadcopter systems in simulation. The results compare different distributed sampling-based optimizers
    and their centralized counterparts. \augustinos{Maybe add different policies.} The scalability of the proposed distributed
    algorithms is demonstrated on a 196-vehicle scenario where a direct application of centralized sampling-based methods is
    shown to be prohibitive.}
}

@article{saravanos2022distributed_mpcs,
    abbr={ICRA 2023 <br> (In Submission)},
    title={Distributed Model Predictive Covariance Steering},
    author={Saravanos, Augustinos D. and Balci, Isin M and Bakolas, Efstathios and Theodorou, Evangelos},
    journal={IEEE International Conference on Robotics and Automation (ICRA) 2023 (In Submission)},
    year={2022},
    selected={true},
    abstract = {This paper proposes Distributed Model Predictive Covariance Steering (DMPCS), a novel method for safe
    multi-robot control under uncertainty. The scope of our approach is to blend covariance steering theory, distributed
    optimization and model predictive control (MPC) into a single methodology that is safe, scalable and decentralized.
    Initially, we pose a problem formulation that uses the Wasserstein distance to steer the state distributions of a
    multi-robot team to desired targets, and probabilistic constraints to ensure safety. We then transform this problem
    into a finite-dimensional optimization one by utilizing a disturbance feedback policy parametrization for covariance
    steering and a tractable approximation of the safety constraints. To solve the latter problem, we derive a decentralized
    consensus-based algorithm using the Alternating Direction Method of Multipliers (ADMM). This method is then extended to
    a receding horizon form, which yields the proposed DMPCS algorithm. Simulation experiments on large-scale problems with
    up to hundreds of robots successfully demonstrate the effectiveness and scalability of DMPCS. Its superior capability
    in achieving safety is also highlighted through a comparison against a standard stochastic MPC approach.}
}

@article{saravanos2022distributed_ddp,
    abbr={IEEE T-RO <br> (In Submission)},
    title={Distributed Differential Dynamic Programming Architectures for Large-Scale Multi-Agent Control},
    author={Saravanos, Augustinos D. and Aoyama, Yuichiro and Zhu, Hongchang and Theodorou, Evangelos A},
    journal={IEEE Transactions on Robotics (In Submission)},
    year={2022},
    selected={true},
    abstract = {In this paper, we propose two novel decentralized optimization frameworks for multi-agent nonlinear
    optimal control problems in robotics. The aim of this work is to suggest architectures that inherit the computational
    efficiency and scalability of Differential Dynamic Programming (DDP) and the distributed nature of the Alternating
    Direction Method of Multipliers (ADMM). In this direction, two frameworks are introduced. The first one called Nested
    Distributed DDP (ND-DDP), is a three-level architecture which employs ADMM for enforcing a consensus between all agents,
    an Augmented Lagrangian layer for satisfying local constraints and DDP as each agentâ€™s optimizer. In the second approach,
    both consensus and local constraints are handled with ADMM, yielding a two-level architecture called Merged Distributed
    DDP (MD-DDP), which further reduces computational complexity. Both frameworks are fully decentralized since all
    computations are parallelizable among the agents and only local communication is necessary. Simulation results that scale
    up to thousands of vehicles and hundreds of drones verify the effectiveness of the methods. Superior scalability to
    large-scale systems against centralized DDP and centralized/decentralized sequential quadratic programming is also illustrated.
    Finally, hardware experiments on a multi-robot platform demonstrate the applicability of the proposed algorithms, while
    highlighting the importance of optimizing for feedback policies to increase robustness against uncertainty. <a href="https://youtu.be/tluvENcWldw" target="_blank">A video
    with all results is available here</a>.},
    video = {https://www.youtube.com/watch?v=tluvENcWldw},
    pdf = {https://arxiv.org/abs/2207.13255}
}

@article{pereira2022decentralized,
    abbr={RSS 2022},
    AUTHOR    = {Marcus A. Pereira* AND Augustinos D. Saravanos* AND Oswin So AND Evangelos A. Theodorou},
    TITLE     = {{Decentralized Safe Multi-agent Stochastic Optimal Control using Deep FBSDEs and ADMM}},
    journal={Robotics: Science and Systems 2022 (*Equal Contribution)},
    YEAR      = {2022},
    ADDRESS   = {New York City, NY, USA},
    DOI       = {10.15607/RSS.2022.XVIII.055},
    selected={true},
    abstract={In this work, we propose a novel safe and scalable decentralized solution for multi-agent control in the
    presence of stochastic disturbances. Safety is mathematically encoded using stochastic control barrier functions and
    safe controls are computed by solving quadratic programs. Decentralization is achieved by augmenting to each agent's
    optimization variables, copy variables, for its neighboring agents. This allows us to decouple the centralized multi-agent
    optimization problem. However, to ensure safety, neighboring agents must agree on what is safe for both of us and this
    creates a need for consensus. To enable safe consensus solutions, we incorporate an ADMM-based approach. Specifically,
    we propose a Merged CADMM-OSQP implicit neural network layer, that solves a mini-batch of both, local quadratic programs
    as well as the overall consensus problem, as a single optimization problem. This layer is embedded within a Deep FBSDEs
    network architecture at every time step, to facilitate end-to-end differentiable, safe and decentralized stochastic optimal
    control. The efficacy of the proposed approach is demonstrated on several challenging multi-robot tasks in simulation.
    By imposing requirements on safety specified by collision avoidance constraints, the safe operation of all agents is
    ensured during the entire training process. We also demonstrate superior scalability in terms of computational and memory
    savings as compared to a centralized approach.},
    video = {https://www.youtube.com/watch?v=qjPLUlaxJos},
    pdf = {http://www.roboticsproceedings.org/rss18/p055.html}
}

@article{pereira2022sim2real,
    abbr={RSS 2022 <br> SRL Workshop},
    title={Sim2Real on the Robotarium Platform Using Decentralized Multi-Agent Safe Deep FBSDEs},
    AUTHOR    = {Marcus A. Pereira* AND Augustinos D. Saravanos* AND Evangelos A. Theodorou},
    journal={Robotics: Science and Systems 2022: Workshop on Scaling Robot Learning (*Equal Contribution)},
    year={2022},
    selected={true},
    abstract = {In this work, we propose a novel sim2real framework for multi-robot control that relies on
    stochastic optimal control theory. Specifically, we use a recently proposed Deep Forward-Backward Stochastic
    Differential Equations (FBSDEs) algorithm to train LSTM-network-based feedback policies in simulation that are
    subsequently deployed directly on real hardware. This particular Deep FBSDE variant is tailored for multi-agent
    systems and ensures safety during the entire training process. Safety is facilitated by employing stochastic
    control barrier functions and decentralization is achieved by an ADMM-based consensus optimization approach. By
    randomizing the initial conditions and inducing noise into the robot's dynamics, the policy is trained to control a
    mini-batch of trajectories to their respective targets on average, while ensuring safety for each trajectory with probability 1.
    We hypothesize that the ability to control an ensemble of trajectories empowers the feedback policy to compensate
    for uncertainty when transferred from simulation to real robots. We test this hypothesis on the Robotarium swarm-robotics
    testbed and successfully demonstrate the completion of tasks as well as safe operation when
    a policy trained in simulation is deployed without any prior real-world experience.  },
    video = {https://www.youtube.com/watch?v=3SvrluLq4ic},
    video2 = {https://www.youtube.com/watch?v=v2tb0Znh6lw},
    pdf = {https://drive.google.com/file/d/1o5BmSE1nQELEX8MEWFxIHtn_usvRGF8G/view}
}

@inproceedings{saravanos2021distributed,
    abbr={RSS 2021},
    AUTHOR    = {Augustinos D. Saravanos AND Alexandros Tsolovikos AND Efstathios Bakolas AND Evangelos Theodorou},
    TITLE     = {{Distributed Covariance Steering with Consensus ADMM for Stochastic Multi-Agent Systems}},
    BOOKTITLE = {Proceedings of Robotics: Science and Systems},
    YEAR      = {2021},
    ADDRESS   = {Virtual},
    DOI       = {10.15607/RSS.2021.XVII.075},
    selected = {true},
    abstract = {In this paper, we address the problem of steering a team of agents under stochastic linear dynamics to
    prescribed final state means and covariances. The agents operate in a common environment where inter-agent constraints
    may also be present. In order for our method to be scalable to large-scale systems and computationally efficient,
    we approach the problem in a distributed control framework using the Alternating Direction Method of Multipliers (ADMM).
    Each agent solves its own covariance steering problem in parallel, while additional copy variables for its closest
    neighbors are introduced to ensure that the inter-agent constraints will be satisfied. The inclusion of these additional
    variables creates a requirement for consensus between original and copy variables that involve the same agent. For this
    reason, we employ a variation of ADMM for consensus optimization. Simulation results on multi-vehicle systems under
    uncertainty with collision avoidance constraints illustrate the effectiveness of our algorithm. The substantially
    improved scalability of our distributed approach with respect to the number of agents is also demonstrated, in comparison
    with an equivalent centralized scheme.},
    video = {https://www.youtube.com/watch?v=RiLQ2P1WXHQ},
    pdf = {http://www.roboticsproceedings.org/rss17/p075.html}

}

@INPROCEEDINGS{aoyama2021receding,
    abbr={CDC 2021},
    author={Aoyama, Yuichiro and Saravanos, Augustinos D. and Theodorou, Evangelos A.},
    booktitle={2021 60th IEEE Conference on Decision and Control (CDC)},
    title={Receding Horizon Differential Dynamic Programming Under Parametric Uncertainty},
    year={2021},
    volume={},
    number={},
    pages={3761-3767},
    doi={10.1109/CDC45484.2021.9683370},
    selected={true},
    abstract = {Generalized Polynomial Chaos (gPC) theory has been widely used for representing parametric uncertainty in
    a system, thanks to its ability to propagate uncertainty evolution. In an optimal control context, gPC can be combined
    with several optimization techniques to achieve a control policy that handles effectively this type of uncertainty.
    Such a suitable method is Differential Dynamic Programming (DDP), leading to an algorithm that inherits the scalability
    to high-dimensional systems and fast convergence nature of the latter. In this paper, we expand this combination aiming
    to acquire probabilistic guarantees on the satisfaction of nonlinear constraints. In particular, we exploit the ability
    of gPC to express higher order moments of the uncertainty distribution - without any Gaussianity assumption - and we
    incorporate chance constraints that lead to expressions involving the state covariance. Furthermore, we demonstrate that
    by implementing our algorithm in a receding horizon fashion, we are able to compute control policies that effectively
    reduce the accumulation of uncertainty on the trajectory. The applicability of our method is verified through simulation
    results on a differential wheeled robot and a quadrotor that perform obstacle avoidance tasks.},
    pdf = {https://ieeexplore.ieee.org/abstract/document/9683370}
}

